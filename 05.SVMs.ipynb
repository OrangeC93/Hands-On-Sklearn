{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. Support Vector Machines\n",
    "> Can be used for regression and classification. Can use multiple kernels for differing preferences on computational complexity and the nonlinearity of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM is a powerful and versetile ML model, capable of performing Classification, Regression, & even outlier detection.\n",
    "- SVMs are particularly suited for complex small-to-medium sized datasets.\n",
    "- This chapter will explain the core concepts of SVMs, how they work, and how to use them.\n",
    "\n",
    "## Linear SVM Classification\n",
    "\n",
    "- The fundamental idea behind SVMs is better explained with the following picture:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/SVM_example.png\" /></div>\n",
    "\n",
    "- The two classes can be separated easily by a straight line (linearly separable).\n",
    "- The left plot shows the decision boundaries of three possible linear classifiers.\n",
    "- The dashed line model is so bad that it doesn't even separate the two groups linearly.\n",
    "- The other two models work perfectly on the plotted training set.\n",
    "    - But their boundaries are so close to the training data points that they'll probably not perform well on unseen data.\n",
    "- In constrast, the model on the right not only separates the training data linearly, it also stays as far as possible from both classes data points.\n",
    "    - Thus, it will likely perform well on unseen data.\n",
    "- You can think of an SVM as fitting the widest possible street (represented by the dashed lines) between the classes.\n",
    "    - This is called **Large Margin Classification**.\n",
    "- Notice that adding more training points off the street **won't effect the decision boundary at all**.\n",
    "- It's fully determined by the data points located at the edge of the street.\n",
    "- These instances are called **support vectors**.\n",
    "- SVMs are also sensitive to feature scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin Classification\n",
    "\n",
    "- If we restrict that all training instances should be off the street, this is called Hard Margin Classification, the problem with it is that is will only with Linearly separated data, and is greatly effected by the presence of outliers.\n",
    "- The following are two example of how outliers can mess-up hard margin classifiers:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/Hard_Margin_Classifier.png\" /></div>\n",
    "\n",
    "- To fix the issue, try to balance finding a wide street with limiting the number of violations.\n",
    "    - This is called **Soft Margin Classification**.\n",
    "- This can be controlled in scikit-learn by the `C` hyper-parameter:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/Soft_to_Hard_Street.png\" /></div>\n",
    "\n",
    "- By increasing `C`, We're increasing the sensitivity of the model to minimize margin violations within the training set.\n",
    "    - Meaning, If you're overfitting, try to reduce the value of the `C` hyper-parameter.\n",
    "- Let's use scikit-learn's SVMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 2), (150,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris['data'].shape\n",
    "\n",
    "X = iris['data'][:,[2,3]]\n",
    "y = (iris['target']==2).astype(np.float64)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf = Pipeline([\n",
    "    ('Scaler', StandardScaler()),\n",
    "    ('Linear_svc', LinearSVC(C=1, loss='hinge'))\n",
    "])\n",
    "\n",
    "svm_clf.fit(X, y)\n",
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unlike Logistic Regression Models (with their sigmoid functions), SVMs do not output probabilities for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NonLinear SVM Classification\n",
    "\n",
    "- Many datasets are not even close to being lienarly separable.\n",
    "- One approach to handly non-linear modeling is to add more features, such as polynomial features.\n",
    "    - In some cases this can result in linearly separable datasets.\n",
    "- The following is an example of an original non-linearly separable dataset with only one feature $x_{1}$ (on the left), and an augmented linearly seprable dataset with an added feature $x_{2}=x_{1}^{2}$: \n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/nonlinear_to_linear.png\" /></div>\n",
    "\n",
    "- Let's implement this idea using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=10, loss='hinge'))\n",
    "])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)\n",
    "polynomial_svm_clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following represents the decision boundaries of the model, because we added polynomial degrees, projected boundaries are now non-linear:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:50%;\" src=\"static/imgs/polynomial_svms.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernels\n",
    "\n",
    "- At a low polynomial degrees, adding features cannot deal with complex datasets.\n",
    "- At a high polynomial degrees, we endup adding a lot of features, resulting in a very complex & slow model.\n",
    "- Fortunately, when using SVMs you can apply an almost miraculous mathematical technique called the kernel trick.\n",
    "    - The kernel trick makes it possible to have the same result as if you added many polynomial features without actually adding them.\n",
    "- Let's test it on the moon dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 SVC(C=5, cache_size=200, class_weight=None, coef0=1,\n",
       "                     decision_function_shape='ovr', degree=3,\n",
       "                     gamma='auto_deprecated', kernel='poly', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_kernel_svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', SVC(kernel='poly', degree=3, coef0=1, C=5))\n",
    "])\n",
    "\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This model trains an SVM classifier using a kernel of third degree features.\n",
    "- If your model is overfitting, you might want to decrease the polynomial degree, and If it's underfitting, it might be a good idea to increase the degree\n",
    "    - `coef0` controls how much the model is influenced by high-degree polynomials vs. low degree polynomials.\n",
    "- The following showcases the previously trained model (on the left) vs. a more complex model of kernel degree 10:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/kernel_trick.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Features\n",
    "\n",
    "- Another technique to tackle non-linear problems is to add features computed using a **similarity function**.\n",
    "    - Which measures how much each instance resembles a particular landmark.\n",
    "- For example, let's take the 1D dataset discussed earlier & add two landmarks to it at $x_{1}=-2$ and $x_{1}=1$, as showcased in the left plot of:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/similarity_measures.png\" /></div>\n",
    "\n",
    "- We defined the similarity function to be the **Gaussian Radial Basis Function (RBF)** with $\\gamma = 0.3$:\n",
    "- $\\phi$ is a bell shaped function varying from 0 ($x$ is very far from $l$) to 1 ($x=l$).\n",
    "    - After defining a similarity function, the new features are the **distances** of each training instance from the landmarks.\n",
    "- As you can see from the plot on the right, the instances become lienarly separable using only distance features.\n",
    "- You may wonder how to select the landmarks\n",
    "    - The simplest approach is to create a landmark at each and every point of the training data.\n",
    "    - Doing that will increase the number of dimensions and will yield a better chance of finding a linear separator.\n",
    "    - The downside is that you will create additional `m` number of features (as rows), which may lead in performance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3, gamma=5,\n",
       "                     kernel='rbf', max_iter=-1, probability=False,\n",
       "                     random_state=None, shrinking=True, tol=0.001,\n",
       "                     verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "])\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's take a look at the predictions space with the training set instances (bottom left is the trained model above), others correspond to different hyper-parameter configurations:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/training_rbfs.png\" /></div>\n",
    "\n",
    "- Increasing $\\gamma$ makes the bell-shaped curve narrower, the decision boundary ends up being more irregular, wiggling around individual instances.\n",
    "    - So $\\gamma$ acts as a regularization hyper-parameter.\n",
    "        - Increasing gamma increases model sensitivity (may lead to overfitting)\n",
    "        - decreasing gamma increases model bias (may lead to underfitting)\n",
    "    - Similar to the `C` hyper-parameter.\n",
    "- Other kernels exist but used much more rarely.\n",
    "- Notes\n",
    "    - You should always try the linear kernel first\n",
    "    - If the training set is not too large, you should also try the gaussian RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Complexity\n",
    "\n",
    "- `LinearSVC` doesn't support the kernel trick and scales with time complexity of $\\mathcal{O}(m \\times n)$.\n",
    "    - The algorithm takes longer if you ask for higher precision.\n",
    "        - Precision is controlled by the tollerance hyper-parameter $\\epsilon$.\n",
    "- `SVC` is based on `libsvm` that supports the kernel trick with complexity between $\\mathcal{O}(m^{2} \\times n)$ & $\\mathcal{O}(m^{3} \\times n)$.\n",
    "    - This means that it gets dreadfuly slow when the training instances count gets big.\n",
    "    - This algorithm is good for small to medium sized datasets.\n",
    "    - It scales well with the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Regression\n",
    "\n",
    "- SVMs also support linear and nonlinear regression.\n",
    "- To move from classification to regression we reverse the objective\n",
    "    - Instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM regression tries to fit **as many instance as possible** on the street while limiting margin violations.\n",
    "- The width of the street is controlled by the hyper-parameter $\\epsilon$, following is an example:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/SVM_regression.png\" /></div>\n",
    "\n",
    "- Adding more instances into the margin doesn't effect the model's predictions; thus the model is said to be $\\epsilon$-insensitive.\n",
    "- Let's implement it using `sklearn`'s `SVR` (after scaling & centering the data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,\n",
       "          intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "          random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To tackle linear regression tasks, we can use a kernelized SVM model.\n",
    "- Let's do it using the polynomial kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, cache_size=200, coef0=0.0, degree=2, epsilon=0.1, gamma='auto',\n",
       "    kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svm_poly_reg = SVR(kernel='poly', \n",
    "                   degree=2, \n",
    "                   C=100, \n",
    "                   epsilon=0.1, \n",
    "                   gamma='auto')\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `LinearSVR` scales linearly with the size of the training set, while `SVR` is much slower (just like `LinearSVC` & `SVC`).\n",
    "\n",
    "```\n",
    "Linear Support Vector Regression.\n",
    "\n",
    "Similar to SVR with parameter kernel=’linear’, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under the Hood\n",
    "\n",
    "- This section explains how SVMs make predictions and how their training algorithms work, starting with Linear SVM classifiers.\n",
    "\n",
    "### Decision Function & Predictions\n",
    "\n",
    "- The linear SVM classifier model predicts the class of a new instance by simply computing $w^{T}x+b = w_1x_1+w_2x_2+\\dots+w_nx_n+b$, If it's positive, then $x$ is labeled $1$, otherwise $x$ is labeled $0$:\n",
    "\n",
    "$$\\hat{y}=\\begin{cases}\n",
    "1,  & \\text{if $w^{T}x+b \\ge 0$} \\\\\n",
    "0, & \\text{if $w^{T}x+b < 0$}\n",
    "\\end{cases}$$\n",
    "\n",
    "- The following showcases the training data points and the activation $w^{T}x+b$:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/SVM_space.png\" /></div>\n",
    "\n",
    "- Training an SVM Classifier is finding a $(w,b)$ that makes the margin as wide as possible while avoiding margin violations (hard margin) or limiting them (soft margin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Objective\n",
    "\n",
    "- We know that the slope of the decision function is $||w||$.\n",
    "- Dividing $||w||$ by $2$ will multiply the margin by $2$.\n",
    "- So we want to minimize $||w||$ to get a large margin.\n",
    "- If we also want to avoid any margin violations then:\n",
    "    - We want the decision function to be greater than 1 for all positive training instances.\n",
    "    - And lower than -1 for all negative training instances.\n",
    "- If we define $t^{(i)}=-1$ for negative instances (if $y^{(i)}=0$) and $t^{(i)}=1$ for positive instances (if $y^{(i)}=1$)\n",
    "    - then we can express this constraint as $t^{(i)}(w^{T}x^{(i)}+b)\\ge 1$\n",
    "- We can therefore express the hard margin linear SVM classifier objective as the following constrained optimization problem:\n",
    "\n",
    "$$\\begin{array}{ll}\n",
    "\\text{minimize}_{(w,b)}  & \\frac{1}{2}w^{T}w \\\\\n",
    "\\text{subject to}& t^{(i)}(w^{T}x^{(i)} + b) \\ge 1; \\forall i \\in \\{1,2,\\dots,m\\}\n",
    "\\end{array}$$\n",
    "\n",
    "- To get the soft margin objective, we need to introduce a slack variable $\\zeta^{(i)} \\ge 0$ for each instance.\n",
    "    - $\\zeta^{(i)}$ measures how much the i-th instance is allowed to violate the margin.\n",
    "- We now have two conflicting objectives, Make the slack variables as small as possible to reduce the margin violations, and make $\\frac{1}{2}w^{T}w$ as small as possible to increase the margin.\n",
    "- This is where the $C$ hyper-parameter comes in: It allows to define the tradeoff between these two objectives.\n",
    "    - This gives us the following contrained optimization problem for Softmargin Linear SVM Classifier Objective:\n",
    "    \n",
    "$$\\begin{array}{ll}\n",
    "\\text{minimize}_{(w,b,\\zeta)}  & \\frac{1}{2}w^{T}w + C\\sum_{i=1}^{m}\\zeta^{(i)} \\\\\n",
    "\\text{subject to}& t^{(i)}(w^{T}x^{(i)} + b) \\ge 1 - \\zeta^{(i)}; \\zeta^{(i)} \\ge 0 \\; \\forall i \\in \\{1,2,\\dots,m\\}\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Programming\n",
    "\n",
    "- The hard/soft margin problems are both quadratic optimization problems with linear constraints.\n",
    "    - Such problems are known as *Quadratic Programming* (QP) problems.\n",
    "- One way to train a hard margin linear SVM classifier is to use an off-the-shelf QP Solver and pass it our parameters.\n",
    "- You can also use a Quadratic Solver to train for the soft margin linear SVM classifier.\n",
    "- To use the kernel trick though, we're going to take a look at a different optimization problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dual Problem\n",
    "\n",
    "- Given a constrained optimization problem (Primal problem) it is possible to express a different but closely related problem (dual problem).\n",
    "- The solution to the dual problem typically gives a lower bound to the solution of the primal problem\n",
    "    - But under some conditions it may have the same solution as the primal problem.\n",
    "        - Luckily, the SVM problem happens to meet these conditions.\n",
    "- Here is the dual problem formulation for the SVM Linear classification optimization problem:\n",
    "\n",
    "$$\\begin{array}{ll}\n",
    "\\text{minimize}_{(\\alpha)}  & \\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha^{(i)}\\alpha^{(j)}t^{(i)}t^{(j)}{x^{(i)}}^{T}x^{(j)} - \\sum_{i=1}^{m}\\alpha^{(i)} \\\\\n",
    "\\text{subject to}& \\alpha^{(i)} \\ge 0; \\forall i \\in \\{1,2,\\dots,m\\}\n",
    "\\end{array}$$\n",
    "\n",
    "- Once you find the solution $\\hat{\\alpha}$ to the previous problem, we use the following to calculate its corresponding $\\hat{w}$ abd $\\hat{b}$:\n",
    "\n",
    "$$\\hat{w}=\\sum_{i=1}^{m}\\hat{w}^{(i)}t^{(i)}x^{(i)} \\\\ \\hat{b} = \\frac{1}{n_s} \\sum_{i=1}^{m}(t^{(i)} - \\hat{w}^{T}x^{(i)})$$\n",
    "\n",
    "- The dual problem is faster to solve when the number of instances is less then the number of features.\n",
    "- More importantly, the dual problem makes the kernel trick possible while the primal problem does not, so what is the kernel trick anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernelized SVMs\n",
    "\n",
    "- If you apply the transformation $\\phi$ to all training instances, then the dual problem will contain the dot product $\\phi(x^{(i)})^{T}\\phi(x^{(j)})$.\n",
    "- But if $\\phi$ is the second degree polynomial transformation then you can replace $\\phi(x^{(i)})^{T}\\phi(x^{(j)})$ with $({x^{(i)}}^{T}x^{(j)})^{2}$.\n",
    "    - Meaning, **We don't need to transform the instances at all**.\n",
    "        - This trick makes for a much simpler computation.\n",
    "- The function $K(a,b)=(a^{T}b)^{2}$ is a 2nd degree polynomial kernel.\n",
    "- In ML, a kernel is a function capable of computing $\\phi(a)^{T}\\phi(b)$ based only on the original vectors $a$ and $b$ without having to compute the original transformation $\\phi$\n",
    "- In the case of the gaussian RBF kernel: $K(a,b)=exp(-\\gamma||a-b||^{2})$, It can be shown that $\\phi$ maps each instance to an infinite dimensional space, so it's a good thing you don't actually need to perform the mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online SVMs\n",
    "\n",
    "- Remember that online learning means learning incrementally, as new instances arrive.\n",
    "- For linear SVMs classifiers, one way to implement online learning is through gradient descent to minimize the cost function derived from the primal objective.\n",
    "- Unfortunately, GD converges much slower than the QP methods.\n",
    "- For large-scale non-linear problems, you may want to consider using neural networks instead.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
